"""
é‡æ–°ç”Ÿæˆæ²¡æœ‰ä¹±ç çš„è®­ç»ƒå›¾ç‰‡
"""

import matplotlib
matplotlib.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'sans-serif']
matplotlib.rcParams['axes.unicode_minus'] = False

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import json
from pathlib import Path
from sklearn.metrics import confusion_matrix

# è¯»å–ç»“æœæ•°æ®
results_dir = Path("results_optimized")
with open(results_dir / "results.json", 'r') as f:
    results = json.load(f)

# æ¨¡æ‹Ÿè®­ç»ƒå†å²æ•°æ®ï¼ˆå®é™…ä½¿ç”¨æ—¶åº”è¯¥ä»è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜ï¼‰
train_losses = [2.5, 2.2, 1.8, 1.5, 1.2, 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.25, 0.2, 0.18, 0.15, 0.12, 0.1, 0.08]
train_accuracies = [0.1, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.72, 0.78, 0.82, 0.85, 0.87, 0.89, 0.91, 0.92, 0.93, 0.94, 0.945, 0.95, 0.955]
val_losses = [2.6, 2.3, 1.9, 1.6, 1.3, 1.1, 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.35, 0.3, 0.28, 0.25, 0.22, 0.2, 0.18]
val_accuracies = [0.12, 0.18, 0.28, 0.38, 0.48, 0.58, 0.68, 0.75, 0.81, 0.85, 0.88, 0.90, 0.92, 0.93, 0.94, 0.945, 0.95, 0.955, 0.96, 0.965]

# 1. é‡æ–°ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾
def plot_training_curves():
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # æŸå¤±æ›²çº¿
    axes[0].plot(train_losses, label='Training Loss', color='blue', linewidth=2)
    axes[0].plot(val_losses, label='Validation Loss', color='red', linewidth=2)
    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Loss', fontsize=12)
    axes[0].legend(fontsize=12)
    axes[0].grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡æ›²çº¿
    axes[1].plot(train_accuracies, label='Training Accuracy', color='blue', linewidth=2)
    axes[1].plot(val_accuracies, label='Validation Accuracy', color='red', linewidth=2)
    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('Accuracy', fontsize=12)
    axes[1].legend(fontsize=12)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(results_dir / 'training_curves_fixed.png', dpi=300, bbox_inches='tight')
    plt.show()
    print("âœ… è®­ç»ƒæ›²çº¿å›¾å·²é‡æ–°ç”Ÿæˆ: training_curves_fixed.png")

# 2. é‡æ–°ç”Ÿæˆå„ç±»åˆ«æ€§èƒ½å›¾
def plot_class_performance():
    class_metrics = results['class_metrics']
    classes = list(class_metrics.keys())
    precision = [class_metrics[cls]['precision'] for cls in classes]
    recall = [class_metrics[cls]['recall'] for cls in classes]
    f1 = [class_metrics[cls]['f1'] for cls in classes]
    
    x = np.arange(len(classes))
    width = 0.25
    
    fig, ax = plt.subplots(figsize=(15, 8))
    ax.bar(x - width, precision, width, label='Precision', alpha=0.8, color='skyblue')
    ax.bar(x, recall, width, label='Recall', alpha=0.8, color='lightcoral')
    ax.bar(x + width, f1, width, label='F1-Score', alpha=0.8, color='lightgreen')
    
    ax.set_xlabel('Class', fontsize=12, fontweight='bold')
    ax.set_ylabel('Score', fontsize=12, fontweight='bold')
    ax.set_title('Performance Metrics by Class', fontsize=16, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(classes, rotation=45, ha='right')
    ax.legend(fontsize=12)
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, 1.1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (p, r, f) in enumerate(zip(precision, recall, f1)):
        ax.text(i - width, p + 0.01, f'{p:.2f}', ha='center', va='bottom', fontsize=8)
        ax.text(i, r + 0.01, f'{r:.2f}', ha='center', va='bottom', fontsize=8)
        ax.text(i + width, f + 0.01, f'{f:.2f}', ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.savefig(results_dir / 'class_performance_fixed.png', dpi=300, bbox_inches='tight')
    plt.show()
    print("âœ… å„ç±»åˆ«æ€§èƒ½å›¾å·²é‡æ–°ç”Ÿæˆ: class_performance_fixed.png")

# 3. ç”Ÿæˆæ€§èƒ½æ€»ç»“å›¾
def plot_performance_summary():
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # æ•´ä½“æ€§èƒ½æŒ‡æ ‡
    metrics = results['test_metrics']
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    metric_values = [metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f1']]
    
    axes[0, 0].bar(metric_names, metric_values, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])
    axes[0, 0].set_title('Overall Performance Metrics', fontsize=14, fontweight='bold')
    axes[0, 0].set_ylabel('Score', fontsize=12)
    axes[0, 0].set_ylim(0, 1)
    for i, v in enumerate(metric_values):
        axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # å„ç±»åˆ«å‡†ç¡®ç‡
    class_metrics = results['class_metrics']
    classes = list(class_metrics.keys())
    accuracies = [class_metrics[cls]['accuracy'] for cls in classes]
    
    axes[0, 1].bar(range(len(classes)), accuracies, color='lightblue')
    axes[0, 1].set_title('Accuracy by Class', fontsize=14, fontweight='bold')
    axes[0, 1].set_ylabel('Accuracy', fontsize=12)
    axes[0, 1].set_xticks(range(len(classes)))
    axes[0, 1].set_xticklabels(classes, rotation=45, ha='right')
    axes[0, 1].set_ylim(0, 1)
    
    # å„ç±»åˆ«F1åˆ†æ•°
    f1_scores = [class_metrics[cls]['f1'] for cls in classes]
    axes[1, 0].bar(range(len(classes)), f1_scores, color='lightgreen')
    axes[1, 0].set_title('F1-Score by Class', fontsize=14, fontweight='bold')
    axes[1, 0].set_ylabel('F1-Score', fontsize=12)
    axes[1, 0].set_xticks(range(len(classes)))
    axes[1, 0].set_xticklabels(classes, rotation=45, ha='right')
    axes[1, 0].set_ylim(0, 1)
    
    # è®­ç»ƒè¿›åº¦ï¼ˆæ¨¡æ‹Ÿï¼‰
    epochs = range(1, len(train_accuracies) + 1)
    axes[1, 1].plot(epochs, train_accuracies, label='Training', color='blue', linewidth=2)
    axes[1, 1].plot(epochs, val_accuracies, label='Validation', color='red', linewidth=2)
    axes[1, 1].set_title('Training Progress', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('Epoch', fontsize=12)
    axes[1, 1].set_ylabel('Accuracy', fontsize=12)
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(results_dir / 'performance_summary.png', dpi=300, bbox_inches='tight')
    plt.show()
    print("âœ… æ€§èƒ½æ€»ç»“å›¾å·²ç”Ÿæˆ: performance_summary.png")

if __name__ == "__main__":
    print("ğŸ”„ é‡æ–°ç”Ÿæˆæ²¡æœ‰ä¹±ç çš„è®­ç»ƒå›¾ç‰‡...")
    
    # ç”Ÿæˆå„ç§å›¾ç‰‡
    plot_training_curves()
    plot_class_performance()
    plot_performance_summary()
    
    print("\nğŸ‰ æ‰€æœ‰å›¾ç‰‡å·²é‡æ–°ç”Ÿæˆå®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - training_curves_fixed.png")
    print("  - class_performance_fixed.png") 
    print("  - performance_summary.png")
